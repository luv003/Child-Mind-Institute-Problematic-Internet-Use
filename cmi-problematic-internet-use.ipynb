{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11c45ac",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T06:08:28.611348Z",
     "iopub.status.busy": "2024-12-19T06:08:28.611056Z",
     "iopub.status.idle": "2024-12-19T06:35:55.629209Z",
     "shell.execute_reply": "2024-12-19T06:35:55.628111Z"
    },
    "papermill": {
     "duration": 1647.024321,
     "end_time": "2024-12-19T06:35:55.631277",
     "exception": false,
     "start_time": "2024-12-19T06:08:28.606956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, LR=0.001: 100%|██████████| 8/8 [02:47<00:00, 20.90s/it, Train Loss=0.8192, Val Loss=0.8665, Val QWK=0.3264]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "Fold 1, LR=0.003:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  12%|█▎        | 1/8 [00:19<02:17, 19.67s/it, Train Loss=0.8391, Val Loss=0.8654, Val QWK=0.3113]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  25%|██▌       | 2/8 [00:39<01:57, 19.64s/it, Train Loss=0.8488, Val Loss=0.8697, Val QWK=0.3169]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  38%|███▊      | 3/8 [00:59<01:38, 19.77s/it, Train Loss=0.8566, Val Loss=0.8753, Val QWK=0.2890]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  50%|█████     | 4/8 [01:18<01:19, 19.77s/it, Train Loss=0.8784, Val Loss=0.8969, Val QWK=0.3264]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  62%|██████▎   | 5/8 [01:38<00:59, 19.75s/it, Train Loss=0.8713, Val Loss=1.0626, Val QWK=0.2670]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  75%|███████▌  | 6/8 [01:58<00:39, 19.75s/it, Train Loss=0.8736, Val Loss=0.9943, Val QWK=0.3061]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003:  88%|████████▊ | 7/8 [02:18<00:19, 19.72s/it, Train Loss=0.8723, Val Loss=0.9344, Val QWK=0.3172]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 1, LR=0.003: 100%|██████████| 8/8 [02:38<00:00, 19.79s/it, Train Loss=0.8931, Val Loss=0.9319, Val QWK=0.2688]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.001: 100%|██████████| 8/8 [02:37<00:00, 19.73s/it, Train Loss=0.8081, Val Loss=0.8367, Val QWK=0.3494]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "Fold 2, LR=0.003:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  12%|█▎        | 1/8 [00:20<02:20, 20.13s/it, Train Loss=0.8458, Val Loss=0.8471, Val QWK=0.3263]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  25%|██▌       | 2/8 [00:39<01:58, 19.76s/it, Train Loss=0.8582, Val Loss=0.8489, Val QWK=0.3361]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  38%|███▊      | 3/8 [00:59<01:39, 19.82s/it, Train Loss=0.8619, Val Loss=0.8662, Val QWK=0.3557]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  50%|█████     | 4/8 [01:19<01:19, 19.87s/it, Train Loss=0.8658, Val Loss=0.8430, Val QWK=0.3868]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  62%|██████▎   | 5/8 [01:39<00:59, 19.81s/it, Train Loss=0.8782, Val Loss=0.8447, Val QWK=0.3670]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  75%|███████▌  | 6/8 [01:59<00:39, 19.87s/it, Train Loss=0.8650, Val Loss=0.8492, Val QWK=0.3793]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003:  88%|████████▊ | 7/8 [02:18<00:19, 19.71s/it, Train Loss=0.8705, Val Loss=1.0134, Val QWK=0.3515]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 2, LR=0.003: 100%|██████████| 8/8 [02:38<00:00, 19.83s/it, Train Loss=0.8526, Val Loss=1.0064, Val QWK=0.3476]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.001: 100%|██████████| 8/8 [02:37<00:00, 19.66s/it, Train Loss=0.8450, Val Loss=0.8246, Val QWK=0.3736]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "Fold 3, LR=0.003:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  12%|█▎        | 1/8 [00:19<02:17, 19.66s/it, Train Loss=0.8784, Val Loss=0.8178, Val QWK=0.3699]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  25%|██▌       | 2/8 [00:39<01:58, 19.70s/it, Train Loss=0.8854, Val Loss=0.8396, Val QWK=0.3861]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  38%|███▊      | 3/8 [00:59<01:38, 19.76s/it, Train Loss=0.8906, Val Loss=0.8219, Val QWK=0.3877]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  50%|█████     | 4/8 [01:19<01:19, 19.83s/it, Train Loss=0.8959, Val Loss=0.8502, Val QWK=0.4075]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  62%|██████▎   | 5/8 [01:38<00:59, 19.67s/it, Train Loss=0.8859, Val Loss=0.8371, Val QWK=0.3773]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  75%|███████▌  | 6/8 [01:58<00:39, 19.68s/it, Train Loss=0.8984, Val Loss=0.8581, Val QWK=0.3235]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003:  88%|████████▊ | 7/8 [02:17<00:19, 19.56s/it, Train Loss=0.8974, Val Loss=0.8553, Val QWK=0.3400]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 3, LR=0.003: 100%|██████████| 8/8 [02:37<00:00, 19.69s/it, Train Loss=0.8941, Val Loss=0.9497, Val QWK=0.3543]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.001: 100%|██████████| 8/8 [02:36<00:00, 19.61s/it, Train Loss=0.8060, Val Loss=0.8410, Val QWK=0.3525]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "Fold 4, LR=0.003:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  12%|█▎        | 1/8 [00:19<02:19, 19.87s/it, Train Loss=0.8288, Val Loss=0.8448, Val QWK=0.3373]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  25%|██▌       | 2/8 [00:39<01:57, 19.53s/it, Train Loss=0.8405, Val Loss=0.8515, Val QWK=0.3350]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  38%|███▊      | 3/8 [00:58<01:37, 19.58s/it, Train Loss=0.8435, Val Loss=0.8383, Val QWK=0.3456]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  50%|█████     | 4/8 [01:18<01:18, 19.56s/it, Train Loss=0.8611, Val Loss=0.8713, Val QWK=0.3859]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  62%|██████▎   | 5/8 [01:38<00:58, 19.61s/it, Train Loss=0.8557, Val Loss=0.8675, Val QWK=0.3293]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  75%|███████▌  | 6/8 [01:58<00:39, 19.77s/it, Train Loss=0.8643, Val Loss=0.9363, Val QWK=0.3180]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003:  88%|████████▊ | 7/8 [02:17<00:19, 19.66s/it, Train Loss=0.8767, Val Loss=0.8559, Val QWK=0.3273]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 4, LR=0.003: 100%|██████████| 8/8 [02:37<00:00, 19.73s/it, Train Loss=0.8526, Val Loss=0.8632, Val QWK=0.3366]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.001: 100%|██████████| 8/8 [02:37<00:00, 19.67s/it, Train Loss=0.8209, Val Loss=0.8863, Val QWK=0.3192]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "Fold 5, LR=0.003:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  12%|█▎        | 1/8 [00:19<02:18, 19.80s/it, Train Loss=0.8943, Val Loss=0.8952, Val QWK=0.3041]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  25%|██▌       | 2/8 [00:39<01:57, 19.57s/it, Train Loss=0.8809, Val Loss=0.9292, Val QWK=0.2913]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  38%|███▊      | 3/8 [00:59<01:39, 19.81s/it, Train Loss=0.8985, Val Loss=0.8898, Val QWK=0.3247]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  50%|█████     | 4/8 [01:19<01:19, 19.85s/it, Train Loss=0.8842, Val Loss=0.8943, Val QWK=0.3321]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  62%|██████▎   | 5/8 [01:38<00:59, 19.70s/it, Train Loss=0.8968, Val Loss=0.9197, Val QWK=0.3283]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  75%|███████▌  | 6/8 [01:58<00:39, 19.79s/it, Train Loss=0.9006, Val Loss=0.9536, Val QWK=0.3216]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003:  88%|████████▊ | 7/8 [02:18<00:19, 19.68s/it, Train Loss=0.9033, Val Loss=0.9625, Val QWK=0.3142]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Fold 5, LR=0.003: 100%|██████████| 8/8 [02:37<00:00, 19.73s/it, Train Loss=0.8924, Val Loss=0.9330, Val QWK=0.3105]\n",
      "<ipython-input-1-0cdddf8dca3f>:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "<ipython-input-1-0cdddf8dca3f>:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "<ipython-input-1-0cdddf8dca3f>:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "<ipython-input-1-0cdddf8dca3f>:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "<ipython-input-1-0cdddf8dca3f>:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "<ipython-input-1-0cdddf8dca3f>:752: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, QuantileTransformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "######################################################\n",
    "# Configuration\n",
    "######################################################\n",
    "WARMUP = True\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 1335\n",
    "EPOCHS = 8\n",
    "LR = [1e-3, 3e-3]\n",
    "DATA_PATH = \"/kaggle/input/child-mind-institute-problematic-internet-use\"\n",
    "DROP_NAN = True\n",
    "CYCLES = 1\n",
    "QWK_WEIGHT = 0.25\n",
    "CE_WEIGHT = 0.75\n",
    "WARMUP_RATIO = [0.0, 1.0]\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Columns selected for training as per previous code\n",
    "COLS = [\n",
    "    \"Basic_Demos-Enroll_Season\", \"CGAS-Season\", \"Physical-Season\", \"Fitness_Endurance-Season\",\n",
    "    \"FGC-Season\", \"BIA-Season\", \"PAQ_C-Season\", \"SDS-Season\", \"PreInt_EduHx-Season\",\n",
    "    \"FGC-FGC_PU\", \"BIA-BIA_SMM\", \"BIA-BIA_BMR\", \"BIA-BIA_FFMI\", \"BIA-BIA_TBW\", \"Basic_Demos-Sex\",\n",
    "    \"BIA-BIA_LDM\", \"Fitness_Endurance-Time_Mins\", \"FGC-FGC_GSND\", \"Basic_Demos-Age\", \"Physical-HeartRate\",\n",
    "    \"FGC-FGC_SRL\", \"Physical-Waist_Circumference\", \"Physical-Systolic_BP\", \"CGAS-CGAS_Score\",\n",
    "    \"BIA-BIA_ECW\", \"PAQ_A-PAQ_A_Total\", \"FGC-FGC_SRR\", \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"SDS-SDS_Total_Raw\", \"FGC-FGC_GSD\", \"PAQ_C-PAQ_C_Total\", \"BIA-BIA_BMI\", \"Fitness_Endurance-Time_Sec\",\n",
    "    \"Physical-Height\", \"SDS-SDS_Total_T\", \"FGC-FGC_CU\", \"Physical-Weight\", \"FGC-FGC_TL\",\n",
    "    \"Physical-Diastolic_BP\", \"Physical-BMI\", \"Fitness_Endurance-Max_Stage\", \"BIA-BIA_FMI\", \"BIA-BIA_BMC\",\n",
    "    \"BIA-BIA_DEE\", \"BIA-BIA_ICW\", \"BIA-BIA_Fat\", \"BIA-BIA_LST\", \"BIA-BIA_Activity_Level_num\",\n",
    "]\n",
    "\n",
    "######################################################\n",
    "# Utility Functions\n",
    "######################################################\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Fix random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def quadratic_weighted_kappa(predictions, targets):\n",
    "    \"\"\"Compute Quadratic Weighted Kappa for given predictions and targets.\"\"\"\n",
    "    return cohen_kappa_score(predictions, targets, weights=\"quadratic\")\n",
    "\n",
    "def threshold_rounder(oof_non_rounded: np.ndarray, thresholds: List[float]) -> np.ndarray:\n",
    "    \"\"\"Round continuous predictions into discrete classes based on thresholds.\"\"\"\n",
    "    return np.where(\n",
    "        oof_non_rounded < thresholds[0],\n",
    "        0,\n",
    "        np.where(\n",
    "            oof_non_rounded < thresholds[1],\n",
    "            1,\n",
    "            np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def evaluate_predictions(thresholds: List[float], y_true: np.ndarray, oof_non_rounded: np.ndarray) -> float:\n",
    "    \"\"\"Evaluate predictions by computing negative QWK for use in optimization.\"\"\"\n",
    "    rounded_p = threshold_rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def create_preprocessor(categorical_features: List[int], numerical_features: List[int]):\n",
    "    \"\"\"Create a ColumnTransformer for preprocessing categorical and numerical features.\"\"\"\n",
    "    # More robust imputations can be done here (like KNNImputer), but here we keep it simple.\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"num\",\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                        (\"scaler\", QuantileTransformer()),\n",
    "                    ]\n",
    "                ),\n",
    "                numerical_features,\n",
    "            ),\n",
    "            (\n",
    "                \"cat\",\n",
    "                Pipeline(\n",
    "                    [\n",
    "                        (\n",
    "                            \"imputer\",\n",
    "                            SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "                        ),\n",
    "                        (\n",
    "                            \"encoder\",\n",
    "                            OrdinalEncoder(\n",
    "                                handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                categorical_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "######################################################\n",
    "# Dataset and Model Classes\n",
    "######################################################\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset to handle hybrid (tabular + time-series) data.\"\"\"\n",
    "\n",
    "    def __init__(self, tabular_data: np.ndarray, ids: np.ndarray, targets: np.ndarray = None, cat_len=9, is_test=False):\n",
    "        self.categorical_data = torch.LongTensor(tabular_data[:, :cat_len])\n",
    "        self.numerical_data = torch.FloatTensor(tabular_data[:, cat_len:-1])\n",
    "        self.ts_indicator = tabular_data[:, -1]\n",
    "        self.ids = ids\n",
    "        self.is_test = is_test\n",
    "        if targets is not None:\n",
    "            self.targets = torch.LongTensor(targets)\n",
    "        else:\n",
    "            self.targets = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numerical_data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        categorical = self.categorical_data[idx]\n",
    "        numerical = self.numerical_data[idx]\n",
    "\n",
    "        # Check if time-series data exists\n",
    "        if self.ts_indicator[idx] == 1:\n",
    "            file_path = os.path.join(\n",
    "                DATA_PATH,\n",
    "                f\"series_{'test' if self.is_test else 'train'}.parquet/id={self.ids[idx]}/part-0.parquet\",\n",
    "            )\n",
    "            if not os.path.exists(file_path):\n",
    "                # Gracefully handle missing files\n",
    "                logger.warning(f\"Time-series file not found for id={self.ids[idx]}, using zeros.\")\n",
    "                time_series = torch.zeros((1, 7))\n",
    "            else:\n",
    "                time_series_df = pd.read_parquet(file_path)\n",
    "                # Basic sanity checks\n",
    "                if len(time_series_df) == 0:\n",
    "                    logger.warning(f\"No time-series data for id={self.ids[idx]}, using zeros.\")\n",
    "                    time_series = torch.zeros((1, 7))\n",
    "                else:\n",
    "                    # Extract relevant columns\n",
    "                    numerical_ts = time_series_df.iloc[:, [1, 2, 3, 4, 5]].values\n",
    "                    timestamp = (time_series_df.iloc[:, 9] / 5000000000).round(3).values\n",
    "                    weekday = time_series_df.iloc[:, 10].values - 1\n",
    "                    # Ensure weekday bounds\n",
    "                    weekday = np.clip(weekday, 0, 6)\n",
    "                    time_series_arr = np.column_stack([numerical_ts, timestamp, weekday])\n",
    "                    time_series = torch.FloatTensor(time_series_arr[::100])\n",
    "        else:\n",
    "            time_series = torch.zeros((1, 7))\n",
    "\n",
    "        if self.targets is not None:\n",
    "            target = self.targets[idx]\n",
    "            return categorical, numerical, time_series, target\n",
    "        return categorical, numerical, time_series\n",
    "\n",
    "def collate_fn(batch: List[Any]):\n",
    "    \"\"\"Collate function for train/validation data loaders.\"\"\"\n",
    "    categorical, numerical, time_series, targets = zip(*batch)\n",
    "    time_series_padded = pad_sequence(time_series, batch_first=True)\n",
    "    return (\n",
    "        torch.stack(categorical),\n",
    "        torch.stack(numerical),\n",
    "        time_series_padded,\n",
    "        torch.stack(targets),\n",
    "    )\n",
    "\n",
    "def collate_fn_test(batch: List[Any]):\n",
    "    \"\"\"Collate function for test data loader.\"\"\"\n",
    "    categorical, numerical, time_series = zip(*batch)\n",
    "    time_series_padded = pad_sequence(time_series, batch_first=True)\n",
    "    return (\n",
    "        torch.stack(categorical),\n",
    "        torch.stack(numerical),\n",
    "        time_series_padded,\n",
    "    )\n",
    "\n",
    "class SimpleTimeSeriesEncoder(nn.Module):\n",
    "    \"\"\"Simple time-series encoder using Conv + LSTM + Embedding for weekday.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.weekday_embedding = nn.Embedding(7, 8)\n",
    "        self.fc = nn.Linear(hidden_dim + 8, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, 7], last col is weekday\n",
    "        x_num, x_cat = x[:, :, :-1], x[:, :, -1].long()\n",
    "\n",
    "        x_num = x_num.transpose(1, 2)\n",
    "        x_num = self.conv(x_num)\n",
    "        x_num = x_num.transpose(1, 2)\n",
    "\n",
    "        _, (h_n, _) = self.lstm(x_num)\n",
    "        x_num = h_n[-1]\n",
    "\n",
    "        x_cat = self.weekday_embedding(x_cat[:, -1])\n",
    "        x_combined = torch.cat([x_num, x_cat], dim=1)\n",
    "        x_combined = self.fc(x_combined)\n",
    "        x_combined = self.layer_norm(x_combined)\n",
    "        return self.dropout(x_combined)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A simple Residual Block for numerical feature encoding.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = 0.7 * out + 0.3 * residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class NumericalEncoder(nn.Module):\n",
    "    \"\"\"Encoder for numerical features using a small CNN + ResidualBlocks.\"\"\"\n",
    "    def __init__(self, numerical_dim, hidden_dim):\n",
    "        super(NumericalEncoder, self).__init__()\n",
    "        self.initial_conv = nn.Conv1d(1, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn_initial = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.res_block1 = ResidualBlock(hidden_dim, hidden_dim)\n",
    "        self.res_block2 = ResidualBlock(hidden_dim, hidden_dim)\n",
    "        self.final_conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1)\n",
    "        self.bn_final = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.fc = nn.Linear(hidden_dim * numerical_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, numerical_dim]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.relu(self.bn_initial(self.initial_conv(x)))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.relu(self.bn_final(self.final_conv(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    \"\"\"Hybrid model combining categorical embeddings, numerical encoding and time-series encoding.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_dims,\n",
    "        numerical_dim,\n",
    "        time_series_dim,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(dim + 1, embedding_dim, padding_idx=dim)\n",
    "                for dim in categorical_dims\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.numerical_encoder = NumericalEncoder(numerical_dim, hidden_dim)\n",
    "        self.time_series_encoder = SimpleTimeSeriesEncoder(\n",
    "            time_series_dim - 1, hidden_dim\n",
    "        )\n",
    "\n",
    "        combined_dim = len(categorical_dims) * embedding_dim + hidden_dim * 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            NumericalEncoder(combined_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(hidden_dim // 2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, categorical, numerical, time_series):\n",
    "        embedded = []\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            clamped_indices = torch.clamp(categorical[:, i], 0, emb.num_embeddings - 1)\n",
    "            embedded.append(emb(clamped_indices))\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "\n",
    "        numerical_features = self.numerical_encoder(numerical)\n",
    "        time_series_features = self.time_series_encoder(time_series)\n",
    "\n",
    "        combined = torch.cat(\n",
    "            [embedded, numerical_features, time_series_features], dim=1\n",
    "        )\n",
    "        return self.classifier(combined)\n",
    "\n",
    "class QuadraticWeightedKappaLoss(nn.Module):\n",
    "    \"\"\"A loss function approximating QWK for training.\"\"\"\n",
    "    def __init__(self, num_classes, epsilon=1e-10):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        weight_mat = torch.zeros(\n",
    "            (self.num_classes, self.num_classes), device=pred.device\n",
    "        )\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                weight_mat[i, j] = (i - j) ** 2\n",
    "\n",
    "        conf_mat = torch.zeros((self.num_classes, self.num_classes), device=pred.device)\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                conf_mat[i, j] = torch.sum((target == i) * pred[:, j])\n",
    "\n",
    "        conf_mat = conf_mat / torch.sum(conf_mat)\n",
    "        row_sum = torch.sum(conf_mat, dim=1)\n",
    "        col_sum = torch.sum(conf_mat, dim=0)\n",
    "        expected = torch.outer(row_sum, col_sum)\n",
    "\n",
    "        numerator = torch.sum(weight_mat * conf_mat)\n",
    "        denominator = torch.sum(weight_mat * expected)\n",
    "        qwk = numerator / (denominator + self.epsilon)\n",
    "\n",
    "        return qwk\n",
    "\n",
    "######################################################\n",
    "# Training and Evaluation Functions\n",
    "######################################################\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    lr,\n",
    "    device,\n",
    "    patience=10,\n",
    "    fold=1,\n",
    "    wur=0.0,\n",
    "    it=0,\n",
    "):\n",
    "    \"\"\"Train and evaluate the model for a single fold.\"\"\"\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    qwk_loss = QuadraticWeightedKappaLoss(num_classes=4)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(wur * total_steps) if WARMUP else 0,\n",
    "        num_training_steps=total_steps,\n",
    "        num_cycles=1,\n",
    "    )\n",
    "\n",
    "    best_val_qwk = 0\n",
    "    epochs_no_improve = 0\n",
    "    pbar = tqdm(range(epochs), desc=f\"Fold {fold}, LR={lr}\")\n",
    "    for _ in pbar:\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for categorical, numerical, time_series, targets in train_loader:\n",
    "            categorical, numerical, time_series, targets = (\n",
    "                categorical.to(device),\n",
    "                numerical.to(device),\n",
    "                time_series.to(device),\n",
    "                targets.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(categorical, numerical, time_series)\n",
    "            loss = CE_WEIGHT * ce_loss(outputs, targets) + QWK_WEIGHT * qwk_loss(\n",
    "                outputs, targets\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for (categorical, numerical, time_series, targets) in val_loader:\n",
    "                categorical, numerical, time_series, targets = (\n",
    "                    categorical.to(device),\n",
    "                    numerical.to(device),\n",
    "                    time_series.to(device),\n",
    "                    targets.to(device),\n",
    "                )\n",
    "                outputs = model(categorical, numerical, time_series)\n",
    "                loss = CE_WEIGHT * ce_loss(outputs, targets) + QWK_WEIGHT * qwk_loss(\n",
    "                    outputs, targets\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "                temp_val_preds = (\n",
    "                    (outputs.softmax(dim=1) * torch.tensor([0, 1, 2, 3], device=device))\n",
    "                    .sum(dim=1)\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "                temp_targets = targets.cpu().numpy()\n",
    "                val_preds.extend(temp_val_preds)\n",
    "                val_targets.extend(temp_targets)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_preds_rounded = np.round(val_preds).astype(int)\n",
    "        val_qwk = quadratic_weighted_kappa(val_targets, val_preds_rounded)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Train Loss\": f\"{train_loss:.4f}\",\n",
    "                \"Val Loss\": f\"{val_loss:.4f}\",\n",
    "                \"Val QWK\": f\"{val_qwk:.4f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if val_qwk > best_val_qwk:\n",
    "            best_val_qwk = val_qwk\n",
    "            torch.save(model.state_dict(), f\"best_model_fold_{fold}_{it}.pth\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            pbar.write(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Load best model and compute OOF predictions\n",
    "    oof_predictions = np.zeros(len(val_loader.dataset))\n",
    "    oof_targets = np.zeros(len(val_loader.dataset))\n",
    "    model.load_state_dict(torch.load(f\"best_model_fold_{fold}_{it}.pth\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i, (categorical, numerical, time_series, targets) in enumerate(val_loader):\n",
    "            categorical, numerical, time_series, targets = (\n",
    "                categorical.to(device),\n",
    "                numerical.to(device),\n",
    "                time_series.to(device),\n",
    "                targets.to(device),\n",
    "            )\n",
    "            outputs = model(categorical, numerical, time_series)\n",
    "            temp_val_preds = (\n",
    "                (outputs.softmax(dim=1) * torch.tensor([0, 1, 2, 3], device=device))\n",
    "                .sum(dim=1)\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            temp_targets = targets.cpu().numpy()\n",
    "            batch_size = targets.size(0)\n",
    "            end_idx = start_idx + batch_size\n",
    "            oof_predictions[start_idx:end_idx] = temp_val_preds\n",
    "            oof_targets[start_idx:end_idx] = temp_targets\n",
    "            start_idx = end_idx\n",
    "\n",
    "    fold_qwk = cohen_kappa_score(\n",
    "        oof_predictions.round(), oof_targets, weights=\"quadratic\"\n",
    "    )\n",
    "    logger.info(f\"Fold {fold} QWK: {fold_qwk:.4f}\")\n",
    "    return fold_qwk, oof_predictions, oof_targets\n",
    "\n",
    "######################################################\n",
    "# Main Training Function\n",
    "######################################################\n",
    "def train_main():\n",
    "    \"\"\"Main training function with cross-validation and threshold optimization.\"\"\"\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "    if DROP_NAN:\n",
    "        # Ensure we have target values\n",
    "        train_df = train_df.dropna(subset=[\"sii\"])\n",
    "    else:\n",
    "        # If not dropping, we can impute or ignore, here we fill with 0\n",
    "        train_df[\"sii\"] = train_df[\"sii\"].fillna(0)\n",
    "\n",
    "    # Basic data checks\n",
    "    if train_df[\"sii\"].isnull().any():\n",
    "        raise ValueError(\"Some target values are still missing!\")\n",
    "\n",
    "    # Extract features and target\n",
    "    tabular_data = train_df[COLS]\n",
    "    targets = train_df[\"sii\"].values\n",
    "    ids = train_df[\"id\"].values\n",
    "\n",
    "    categorical_features = list(range(9))\n",
    "    numerical_features = list(range(9, len(COLS)))\n",
    "\n",
    "    # Stratified split\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    fold_qwks = []\n",
    "    all_oof_predictions = np.zeros(len(targets))\n",
    "    all_oof_targets = np.zeros(len(targets))\n",
    "    preprocessors = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Training on device: {device}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(tabular_data, targets), 1):\n",
    "        logger.info(f\"Starting fold {fold}\")\n",
    "        preprocessor = create_preprocessor(categorical_features, numerical_features)\n",
    "\n",
    "        fold_train_data = tabular_data.iloc[train_idx]\n",
    "        fold_val_data = tabular_data.iloc[val_idx]\n",
    "\n",
    "        tabular_train = preprocessor.fit_transform(fold_train_data)\n",
    "        tabular_val = preprocessor.transform(fold_val_data)\n",
    "\n",
    "        preprocessors.append(preprocessor)\n",
    "\n",
    "        ids_train, ids_val = ids[train_idx], ids[val_idx]\n",
    "        y_train, y_val = targets[train_idx], targets[val_idx]\n",
    "\n",
    "        # Check time-series data existence\n",
    "        def ts_exists(x, mode='train'):\n",
    "            path = os.path.join(DATA_PATH, f\"series_{mode}.parquet/id={x}/part-0.parquet\")\n",
    "            return 1 if os.path.exists(path) else 0\n",
    "\n",
    "        train_ts_indicator = np.array([ts_exists(x, 'train') for x in ids_train]).reshape(-1, 1)\n",
    "        val_ts_indicator = np.array([ts_exists(x, 'train') for x in ids_val]).reshape(-1, 1)\n",
    "\n",
    "        tabular_train = np.column_stack([tabular_train, train_ts_indicator])\n",
    "        tabular_val = np.column_stack([tabular_val, val_ts_indicator])\n",
    "\n",
    "        categorical_dims = [\n",
    "            len(\n",
    "                preprocessor.named_transformers_[\"cat\"]\n",
    "                .named_steps[\"encoder\"]\n",
    "                .categories_[i]\n",
    "            )\n",
    "            for i in range(len(categorical_features))\n",
    "        ]\n",
    "\n",
    "        train_dataset = HybridDataset(tabular_train, ids_train, y_train)\n",
    "        val_dataset = HybridDataset(tabular_val, ids_val, y_val)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "        model = HybridModel(\n",
    "            categorical_dims=categorical_dims,\n",
    "            numerical_dim=tabular_train.shape[1] - len(categorical_dims) - 1,\n",
    "            time_series_dim=7,\n",
    "            embedding_dim=8,\n",
    "            hidden_dim=128,\n",
    "            num_classes=4,\n",
    "        ).to(device)\n",
    "\n",
    "        # Train first phase\n",
    "        fold_qwk_1, oof_predictions_1, oof_targets_1 = train_and_evaluate(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=EPOCHS,\n",
    "            lr=LR[0],\n",
    "            device=device,\n",
    "            patience=15,\n",
    "            fold=fold,\n",
    "            wur=WARMUP_RATIO[0],\n",
    "            it=0,\n",
    "        )\n",
    "\n",
    "        # Train second phase (with different LR and warmup)\n",
    "        fold_qwk_2, oof_predictions_2, oof_targets_2 = train_and_evaluate(\n",
    "            copy.deepcopy(model),\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=EPOCHS,\n",
    "            lr=LR[1],\n",
    "            device=device,\n",
    "            patience=15,\n",
    "            fold=fold,\n",
    "            wur=WARMUP_RATIO[1],\n",
    "            it=1,\n",
    "        )\n",
    "\n",
    "        # Combine predictions from two training runs\n",
    "        combined_predictions = (oof_predictions_1 + oof_predictions_2) / 2\n",
    "        combined_targets = (oof_targets_1 + oof_targets_2) / 2\n",
    "\n",
    "        fold_qwks.extend([fold_qwk_1, fold_qwk_2])\n",
    "        all_oof_predictions[val_idx] = combined_predictions\n",
    "        all_oof_targets[val_idx] = combined_targets\n",
    "\n",
    "    # Optimize thresholds on OOF predictions\n",
    "    Kappa_optimizer = minimize(\n",
    "        evaluate_predictions,\n",
    "        x0=[0.5, 1.5, 2.5],\n",
    "        args=(all_oof_targets, all_oof_predictions),\n",
    "        method=\"Nelder-Mead\",\n",
    "    )\n",
    "\n",
    "    if not Kappa_optimizer.success:\n",
    "        logger.warning(\"Threshold optimization did not converge. Using default thresholds.\")\n",
    "        optimal_thresholds = [0.5, 1.5, 2.5]\n",
    "    else:\n",
    "        optimal_thresholds = Kappa_optimizer.x\n",
    "\n",
    "    oof_tuned = threshold_rounder(all_oof_predictions, optimal_thresholds)\n",
    "    kappa_optimized = quadratic_weighted_kappa(all_oof_targets, oof_tuned)\n",
    "\n",
    "    default_thresholds = [0.5, 1.5, 2.5]\n",
    "    oof_not_tuned = threshold_rounder(all_oof_predictions, default_thresholds)\n",
    "    oof_qwk = quadratic_weighted_kappa(all_oof_targets, oof_not_tuned)\n",
    "\n",
    "    logger.info(f\"Mean of fold QWKs: {np.mean(fold_qwks):.4f}\")\n",
    "    logger.info(f\"OOF QWK (not optimized): {oof_qwk:.4f}\")\n",
    "    logger.info(f\"OOF QWK (optimized): {kappa_optimized:.4f}\")\n",
    "    logger.info(f\"Optimal thresholds: {optimal_thresholds}\")\n",
    "\n",
    "    return preprocessors, optimal_thresholds\n",
    "\n",
    "######################################################\n",
    "# LightGBM Ensemble (Optional)\n",
    "######################################################\n",
    "def train_lightgbm(train_df: pd.DataFrame, preprocessors, targets: np.ndarray):\n",
    "    \"\"\"Train a LightGBM model on the same features as a fallback or ensemble model.\"\"\"\n",
    "    # Use the first preprocessor as a reference (assuming they are quite similar after fit)\n",
    "    preprocessor = preprocessors[0]\n",
    "\n",
    "    # Preprocess data fully\n",
    "    tab_data = preprocessor.transform(train_df[COLS])\n",
    "    # No time-series indicator here for simplicity\n",
    "    # You may want to incorporate it if it's a known feature\n",
    "    # For demonstration, we just assume no TS data used in LightGBM\n",
    "\n",
    "    # Basic LGBM model\n",
    "    lgb_params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": 4,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": RANDOM_STATE\n",
    "    }\n",
    "    lgb_train = lgb.Dataset(tab_data, label=targets)\n",
    "    model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n",
    "    return model\n",
    "\n",
    "######################################################\n",
    "# Inference Function\n",
    "######################################################\n",
    "def inference(preprocessors, optimal_thresholds):\n",
    "    \"\"\"Perform inference on the test set using saved models and ensemble the predictions.\"\"\"\n",
    "    test_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "    tabular_data = test_df[COLS]\n",
    "    ids = test_df[\"id\"].values\n",
    "\n",
    "    # Check for time-series indicator\n",
    "    def ts_exists(x, mode='test'):\n",
    "        path = os.path.join(DATA_PATH, f\"series_{mode}.parquet/id={x}/part-0.parquet\")\n",
    "        return 1 if os.path.exists(path) else 0\n",
    "\n",
    "    ts_indicator = np.array([ts_exists(x, 'test') for x in ids]).reshape(-1, 1)\n",
    "\n",
    "    # Predictions from neural nets\n",
    "    test_predictions_nn = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(\"Starting inference with neural network models.\")\n",
    "\n",
    "    for fold in range(N_SPLITS):\n",
    "        # Preprocess test data using fold-specific preprocessor\n",
    "        tabular_data_processed = preprocessors[fold].transform(tabular_data)\n",
    "        tabular_data_processed = np.column_stack([tabular_data_processed, ts_indicator])\n",
    "\n",
    "        # Same structure as training dataset\n",
    "        categorical_features = list(range(9))\n",
    "        categorical_dims = [\n",
    "            len(\n",
    "                preprocessors[fold].named_transformers_[\"cat\"]\n",
    "                .named_steps[\"encoder\"]\n",
    "                .categories_[i]\n",
    "            )\n",
    "            for i in range(len(categorical_features))\n",
    "        ]\n",
    "\n",
    "        test_dataset = HybridDataset(tabular_data_processed, ids, is_test=True, cat_len=9)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn_test,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "        # Rebuild model with same structure\n",
    "        model = HybridModel(\n",
    "            categorical_dims=categorical_dims,\n",
    "            numerical_dim=tabular_data_processed.shape[1] - len(categorical_dims) - 1,\n",
    "            time_series_dim=7,\n",
    "            embedding_dim=8,\n",
    "            hidden_dim=128,\n",
    "            num_classes=4,\n",
    "        ).to(device)\n",
    "\n",
    "        fold_preds = []\n",
    "        # Ensemble from both training phases (two saved models per fold)\n",
    "        for it in range(2):\n",
    "            model_path = f\"best_model_fold_{fold+1}_{it}.pth\"\n",
    "            if not os.path.exists(model_path):\n",
    "                logger.warning(f\"Model checkpoint missing: {model_path}\")\n",
    "                continue\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.eval()\n",
    "            preds_tmp = []\n",
    "            with torch.no_grad():\n",
    "                for categorical, numerical, time_series in test_loader:\n",
    "                    categorical, numerical, time_series = (\n",
    "                        categorical.to(device),\n",
    "                        numerical.to(device),\n",
    "                        time_series.to(device),\n",
    "                    )\n",
    "                    outputs = model(categorical, numerical, time_series)\n",
    "                    preds_tmp.extend(\n",
    "                        (outputs.softmax(dim=1) * torch.tensor([0, 1, 2, 3], device=device))\n",
    "                        .sum(dim=1)\n",
    "                        .cpu()\n",
    "                        .numpy()\n",
    "                    )\n",
    "            fold_preds.append(preds_tmp)\n",
    "\n",
    "        if len(fold_preds) > 0:\n",
    "            # Average predictions from the two model variants for this fold\n",
    "            fold_preds = np.mean(fold_preds, axis=0)\n",
    "            test_predictions_nn.append(fold_preds)\n",
    "\n",
    "    # Average predictions across folds\n",
    "    if len(test_predictions_nn) == 0:\n",
    "        logger.error(\"No neural network predictions were generated.\")\n",
    "        test_predictions_nn_final = np.zeros(len(test_df))\n",
    "    else:\n",
    "        test_predictions_nn_final = np.mean(test_predictions_nn, axis=0)\n",
    "\n",
    "    # Ensemble with LightGBM (Optional)\n",
    "    # Load training data again and train LGBM\n",
    "    train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "    if DROP_NAN:\n",
    "        train_df = train_df.dropna(subset=[\"sii\"])\n",
    "    else:\n",
    "        train_df[\"sii\"] = train_df[\"sii\"].fillna(0)\n",
    "    targets = train_df[\"sii\"].values\n",
    "\n",
    "    lgb_model = train_lightgbm(train_df, preprocessors, targets)\n",
    "    tabular_test_processed = preprocessors[0].transform(tabular_data)\n",
    "    lgb_preds = lgb_model.predict(tabular_test_processed)\n",
    "    # Convert LGBM multiclass probs to a weighted sum\n",
    "    lgb_preds_cont = (lgb_preds * np.array([0,1,2,3])).sum(axis=1)\n",
    "\n",
    "    # Ensemble neural net and LGBM predictions by simple averaging\n",
    "    final_predictions_cont = (test_predictions_nn_final + lgb_preds_cont) / 2\n",
    "\n",
    "    # Threshold and round\n",
    "    final_predictions_class = threshold_rounder(final_predictions_cont, optimal_thresholds)\n",
    "    test_df[\"sii\"] = final_predictions_class\n",
    "    test_df[[\"id\", \"sii\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    logger.info(\"Inference complete. Submission saved to submission.csv.\")\n",
    "\n",
    "######################################################\n",
    "# Main Execution\n",
    "######################################################\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(RANDOM_STATE)\n",
    "    preprocessors, optimal_thresholds = train_main()\n",
    "    inference(preprocessors, optimal_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a9156",
   "metadata": {
    "papermill": {
     "duration": 0.012275,
     "end_time": "2024-12-19T06:35:55.657221",
     "exception": false,
     "start_time": "2024-12-19T06:35:55.644946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1652.555782,
   "end_time": "2024-12-19T06:35:58.991376",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-19T06:08:26.435594",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
